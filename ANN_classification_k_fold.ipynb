{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_classification_k_fold.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP0lvEBkjTpH",
        "colab_type": "code",
        "outputId": "1a289a59-587a-4693-a385-42dbae5cbf5e",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "import io\n",
        "data=pd.read_csv(io.StringIO(uploaded['data.csv'].decode('utf-8')))\n",
        "\n",
        "import pandas as pd\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-63da4a5d-4a6f-4119-8bc5-5679d539f134\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-63da4a5d-4a6f-4119-8bc5-5679d539f134\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.csv to data (1).csv\n",
            "User uploaded file \"data.csv\" with length 125204 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zasCuIavm2GQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRLK3-UJm0C1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn8VGrG2p8g_",
        "colab_type": "code",
        "outputId": "c3e6c788-ef8f-4598-e72b-3eeb8141ff24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ79IFFAqJ6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del data['Unnamed: 32']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF447yPOqnjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "X = data.iloc[:,2:].values\n",
        "y = data.iloc[:,1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtmHRaMIy1qP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoding our labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "Labelencoder = LabelEncoder()\n",
        "y = Labelencoder.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow3OJulBzX-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size  = 0.1,random_state = 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hstNJjSczuy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalizing our data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5etQ1t0h1Jjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "8b42e550-f480-4be7-e8c2-a88df36054b0"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.52787029,  2.49821982, -0.59939466, ..., -1.74713139,\n",
              "        -0.79044533, -0.91054389],\n",
              "       [-0.55333608,  0.29431013, -0.60759343, ..., -0.62275667,\n",
              "        -0.33646358, -0.83551633],\n",
              "       [ 2.15452653,  0.40392257,  2.26525805, ...,  1.03846122,\n",
              "        -0.11504791,  0.26488788],\n",
              "       ...,\n",
              "       [-1.3297598 , -0.21876938, -1.32088704, ..., -0.98271999,\n",
              "        -0.718764  , -0.13637062],\n",
              "       [-1.24940108, -0.24209117, -1.2835826 , ..., -1.74713139,\n",
              "        -1.58690456, -1.01280367],\n",
              "       [-0.74291476,  1.08958336, -0.71827692, ..., -0.2865488 ,\n",
              "        -1.26354211,  0.19486216]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO1IJct81Qat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "faba2cf2-39d9-4434-88d8-f3076d9d57f2"
      },
      "source": [
        "X_test"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.19207516,  0.21655218, -0.11542614, ...,  1.60315155,\n",
              "         1.38812047,  1.39561107],\n",
              "       [-0.24797128,  1.29875111, -0.31114085, ..., -0.76400211,\n",
              "        -0.79834577, -0.93437281],\n",
              "       [-0.00967731, -0.89539007, -0.07447263, ..., -0.40317641,\n",
              "        -1.38855752, -0.97661799],\n",
              "       ...,\n",
              "       [-0.46861385,  0.07241152, -0.46935075, ..., -0.49962508,\n",
              "        -0.33652424, -0.40576652],\n",
              "       [-0.75986205, -0.81073603, -0.78878818, ..., -0.67538685,\n",
              "        -0.01842311,  0.13529668],\n",
              "       [-0.80399056,  1.71744542, -0.84310547, ..., -1.40405566,\n",
              "        -0.02225565, -0.77243298]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk7sIFwl1T1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9ff55496-e60c-4f18-b108-8246814d30ef"
      },
      "source": [
        "! pip install keras"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i_v6KE11dR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "cd5c1fa7-64ed-4c0d-bda0-450bd06571f2"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv2BthDJ1tvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "b4ff65ab-678c-4009-b039-f5ab77882346"
      },
      "source": [
        "#adding the input and first hidden layer\n",
        "model = Sequential()\n",
        "model.add(Dense(output_dim = 16,init = 'uniform',activation = 'relu',input_dim = 30))\n",
        "\n",
        "#adding the second hidden layer\n",
        "model.add(Dense(output_dim = 16,init = 'uniform',activation = 'relu'))\n",
        "\n",
        "#adding the output layer\n",
        "model.add(Dense(output_dim = 1,init = 'uniform' ,activation = 'sigmoid'))\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkcDI4Zr20gc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "81ac0422-73b9-4749-b306-71f32b7c23ba"
      },
      "source": [
        "model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ8TBm953wIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d83f70e-1dce-4227-d121-0c6c28a1200c"
      },
      "source": [
        "model.fit(X,y,batch_size = 100,epochs = 150)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "569/569 [==============================] - 1s 1ms/step - loss: 0.6784 - acc: 0.3726\n",
            "Epoch 2/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.6690 - acc: 0.3726\n",
            "Epoch 3/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.6590 - acc: 0.3726\n",
            "Epoch 4/150\n",
            "569/569 [==============================] - 0s 32us/step - loss: 0.6458 - acc: 0.3726\n",
            "Epoch 5/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.6264 - acc: 0.4974\n",
            "Epoch 6/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.6001 - acc: 0.7293\n",
            "Epoch 7/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.5642 - acc: 0.8260\n",
            "Epoch 8/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.5233 - acc: 0.9104\n",
            "Epoch 9/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.4765 - acc: 0.9086\n",
            "Epoch 10/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.4263 - acc: 0.9156\n",
            "Epoch 11/150\n",
            "569/569 [==============================] - 0s 36us/step - loss: 0.3796 - acc: 0.9033\n",
            "Epoch 12/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.3364 - acc: 0.9051\n",
            "Epoch 13/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.3017 - acc: 0.9086\n",
            "Epoch 14/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.2760 - acc: 0.9156\n",
            "Epoch 15/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.2589 - acc: 0.9069\n",
            "Epoch 16/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.2536 - acc: 0.9016\n",
            "Epoch 17/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.2399 - acc: 0.9139\n",
            "Epoch 18/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.2290 - acc: 0.9051\n",
            "Epoch 19/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.2304 - acc: 0.9139\n",
            "Epoch 20/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.2344 - acc: 0.9104\n",
            "Epoch 21/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.2307 - acc: 0.9209\n",
            "Epoch 22/150\n",
            "569/569 [==============================] - 0s 31us/step - loss: 0.2443 - acc: 0.9086\n",
            "Epoch 23/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.2225 - acc: 0.9104\n",
            "Epoch 24/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.2121 - acc: 0.9244\n",
            "Epoch 25/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.2149 - acc: 0.9174\n",
            "Epoch 26/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.2255 - acc: 0.9156\n",
            "Epoch 27/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.2145 - acc: 0.9139\n",
            "Epoch 28/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.2249 - acc: 0.9033\n",
            "Epoch 29/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.2167 - acc: 0.9104\n",
            "Epoch 30/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.2156 - acc: 0.9209\n",
            "Epoch 31/150\n",
            "569/569 [==============================] - 0s 28us/step - loss: 0.2146 - acc: 0.9139\n",
            "Epoch 32/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.2070 - acc: 0.9209\n",
            "Epoch 33/150\n",
            "569/569 [==============================] - 0s 32us/step - loss: 0.2054 - acc: 0.9209\n",
            "Epoch 34/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.2023 - acc: 0.9139\n",
            "Epoch 35/150\n",
            "569/569 [==============================] - 0s 28us/step - loss: 0.2014 - acc: 0.9227\n",
            "Epoch 36/150\n",
            "569/569 [==============================] - 0s 30us/step - loss: 0.2048 - acc: 0.9192\n",
            "Epoch 37/150\n",
            "569/569 [==============================] - 0s 28us/step - loss: 0.2019 - acc: 0.9227\n",
            "Epoch 38/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.2023 - acc: 0.9174\n",
            "Epoch 39/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.2001 - acc: 0.9262\n",
            "Epoch 40/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.2049 - acc: 0.9174\n",
            "Epoch 41/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.2082 - acc: 0.9156\n",
            "Epoch 42/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.2015 - acc: 0.9174\n",
            "Epoch 43/150\n",
            "569/569 [==============================] - 0s 35us/step - loss: 0.2081 - acc: 0.9156\n",
            "Epoch 44/150\n",
            "569/569 [==============================] - 0s 31us/step - loss: 0.2033 - acc: 0.9297\n",
            "Epoch 45/150\n",
            "569/569 [==============================] - 0s 34us/step - loss: 0.1957 - acc: 0.9174\n",
            "Epoch 46/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.2021 - acc: 0.9244\n",
            "Epoch 47/150\n",
            "569/569 [==============================] - 0s 28us/step - loss: 0.1991 - acc: 0.9192\n",
            "Epoch 48/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.1930 - acc: 0.9279\n",
            "Epoch 49/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.1960 - acc: 0.9209\n",
            "Epoch 50/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1978 - acc: 0.9227\n",
            "Epoch 51/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.2034 - acc: 0.9227\n",
            "Epoch 52/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.2030 - acc: 0.9174\n",
            "Epoch 53/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1977 - acc: 0.9227\n",
            "Epoch 54/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.2050 - acc: 0.9192\n",
            "Epoch 55/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1966 - acc: 0.9227\n",
            "Epoch 56/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.1860 - acc: 0.9244\n",
            "Epoch 57/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.2077 - acc: 0.9209\n",
            "Epoch 58/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.2082 - acc: 0.9192\n",
            "Epoch 59/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.1960 - acc: 0.9227\n",
            "Epoch 60/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1864 - acc: 0.9244\n",
            "Epoch 61/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1843 - acc: 0.9279\n",
            "Epoch 62/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1882 - acc: 0.9244\n",
            "Epoch 63/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1874 - acc: 0.9297\n",
            "Epoch 64/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1837 - acc: 0.9262\n",
            "Epoch 65/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1880 - acc: 0.9279\n",
            "Epoch 66/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1860 - acc: 0.9244\n",
            "Epoch 67/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1924 - acc: 0.9262\n",
            "Epoch 68/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1983 - acc: 0.9174\n",
            "Epoch 69/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1824 - acc: 0.9297\n",
            "Epoch 70/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1822 - acc: 0.9244\n",
            "Epoch 71/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1819 - acc: 0.9244\n",
            "Epoch 72/150\n",
            "569/569 [==============================] - 0s 28us/step - loss: 0.1805 - acc: 0.9244\n",
            "Epoch 73/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1789 - acc: 0.9262\n",
            "Epoch 74/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1787 - acc: 0.9297\n",
            "Epoch 75/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1819 - acc: 0.9297\n",
            "Epoch 76/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1785 - acc: 0.9244\n",
            "Epoch 77/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1775 - acc: 0.9244\n",
            "Epoch 78/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1774 - acc: 0.9279\n",
            "Epoch 79/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1867 - acc: 0.9244\n",
            "Epoch 80/150\n",
            "569/569 [==============================] - 0s 34us/step - loss: 0.1801 - acc: 0.9279\n",
            "Epoch 81/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.1763 - acc: 0.9297\n",
            "Epoch 82/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1744 - acc: 0.9262\n",
            "Epoch 83/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1733 - acc: 0.9262\n",
            "Epoch 84/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1752 - acc: 0.9315\n",
            "Epoch 85/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1787 - acc: 0.9315\n",
            "Epoch 86/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1764 - acc: 0.9367\n",
            "Epoch 87/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1790 - acc: 0.9279\n",
            "Epoch 88/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1798 - acc: 0.9262\n",
            "Epoch 89/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1897 - acc: 0.9192\n",
            "Epoch 90/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1833 - acc: 0.9209\n",
            "Epoch 91/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1753 - acc: 0.9332\n",
            "Epoch 92/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1884 - acc: 0.9174\n",
            "Epoch 93/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1800 - acc: 0.9279\n",
            "Epoch 94/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1809 - acc: 0.9279\n",
            "Epoch 95/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1741 - acc: 0.9332\n",
            "Epoch 96/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1718 - acc: 0.9332\n",
            "Epoch 97/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1693 - acc: 0.9279\n",
            "Epoch 98/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.1683 - acc: 0.9297\n",
            "Epoch 99/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1743 - acc: 0.9244\n",
            "Epoch 100/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1746 - acc: 0.9297\n",
            "Epoch 101/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1663 - acc: 0.9297\n",
            "Epoch 102/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1653 - acc: 0.9315\n",
            "Epoch 103/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1654 - acc: 0.9315\n",
            "Epoch 104/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1658 - acc: 0.9367\n",
            "Epoch 105/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1671 - acc: 0.9262\n",
            "Epoch 106/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.1650 - acc: 0.9315\n",
            "Epoch 107/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1775 - acc: 0.9262\n",
            "Epoch 108/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1709 - acc: 0.9385\n",
            "Epoch 109/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1683 - acc: 0.9332\n",
            "Epoch 110/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1678 - acc: 0.9332\n",
            "Epoch 111/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1618 - acc: 0.9332\n",
            "Epoch 112/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1629 - acc: 0.9262\n",
            "Epoch 113/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1662 - acc: 0.9367\n",
            "Epoch 114/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1728 - acc: 0.9315\n",
            "Epoch 115/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1681 - acc: 0.9385\n",
            "Epoch 116/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.1666 - acc: 0.9350\n",
            "Epoch 117/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1623 - acc: 0.9367\n",
            "Epoch 118/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1624 - acc: 0.9367\n",
            "Epoch 119/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1669 - acc: 0.9227\n",
            "Epoch 120/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1672 - acc: 0.9315\n",
            "Epoch 121/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1608 - acc: 0.9297\n",
            "Epoch 122/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1633 - acc: 0.9367\n",
            "Epoch 123/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1641 - acc: 0.9315\n",
            "Epoch 124/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1595 - acc: 0.9350\n",
            "Epoch 125/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1676 - acc: 0.9367\n",
            "Epoch 126/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1672 - acc: 0.9227\n",
            "Epoch 127/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.1875 - acc: 0.9279\n",
            "Epoch 128/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1729 - acc: 0.9279\n",
            "Epoch 129/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1633 - acc: 0.9367\n",
            "Epoch 130/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1628 - acc: 0.9367\n",
            "Epoch 131/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.1678 - acc: 0.9297\n",
            "Epoch 132/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1564 - acc: 0.9438\n",
            "Epoch 133/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1555 - acc: 0.9367\n",
            "Epoch 134/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1622 - acc: 0.9332\n",
            "Epoch 135/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1541 - acc: 0.9367\n",
            "Epoch 136/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1530 - acc: 0.9385\n",
            "Epoch 137/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1547 - acc: 0.9385\n",
            "Epoch 138/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.1527 - acc: 0.9385\n",
            "Epoch 139/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1527 - acc: 0.9367\n",
            "Epoch 140/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1529 - acc: 0.9385\n",
            "Epoch 141/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1574 - acc: 0.9315\n",
            "Epoch 142/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1519 - acc: 0.9332\n",
            "Epoch 143/150\n",
            "569/569 [==============================] - 0s 18us/step - loss: 0.1571 - acc: 0.9367\n",
            "Epoch 144/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1510 - acc: 0.9402\n",
            "Epoch 145/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1525 - acc: 0.9350\n",
            "Epoch 146/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.1580 - acc: 0.9367\n",
            "Epoch 147/150\n",
            "569/569 [==============================] - 0s 33us/step - loss: 0.1497 - acc: 0.9438\n",
            "Epoch 148/150\n",
            "569/569 [==============================] - 0s 32us/step - loss: 0.1516 - acc: 0.9350\n",
            "Epoch 149/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.1525 - acc: 0.9350\n",
            "Epoch 150/150\n",
            "569/569 [==============================] - 0s 28us/step - loss: 0.1488 - acc: 0.9402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0fc4ea6d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF7AVelW4SmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwZgzvyp4iVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXTIWm0A4nKl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "d486df7e-5d43-4990-f7c7-e8410680836b"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cm,annot=True)\n",
        "plt.savefig('h.png')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD4CAYAAACt8i4nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntAAQGEELAIERtAAQGEELAIH9LyqviLxzhhf+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S560UXay5Ubs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "806fcc50-37cd-4ebb-dee0-c9e9cc25a883"
      },
      "source": [
        "(29+16)/(29+16+12)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7894736842105263"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H9lMcIu6yoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9d23998-2746-46be-d1ff-f26d74e57517"
      },
      "source": [
        "'''K_fold'''"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'K_fold'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSHLY_proYUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9MndL-RqmmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-84cVq4prvVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def built_classifier():\n",
        "  classifier = Sequential()\n",
        "  classifier.add(Dense(output_dim=16, init='uniform', activation='relu',input_dim=30))\n",
        "  classifier.add(Dense(output_dim=16, init='uniform', activation='relu'))\n",
        "  classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
        "  classifier.compile(optimizer=\"Adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "classifier = KerasClassifier(build_fn = built_classifier, batch_size = 100, epochs=100)\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y=y_train, cv=10, n_jobs =-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dtLj0nqrxgY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e3dd7d22-e094-4117-b026-2b2e2f34f2f4"
      },
      "source": [
        "accuracies"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.94230771, 0.96153843, 1.        , 0.98039216, 0.96078432,\n",
              "       0.94117647, 0.98039216, 0.98039216, 1.        , 0.98039216])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik1r1UbdsadJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d22a2327-92c8-463d-c28f-eb7fbd42745f"
      },
      "source": [
        "accuracies.mean()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9727375566959381"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26mlETBBsa0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#so using kflod we achieve training accuracy upto 97%"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}