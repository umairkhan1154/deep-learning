{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP0lvEBkjTpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn8VGrG2p8g_",
        "colab_type": "code",
        "outputId": "26cffd16-5448-4d8b-a151-f79b21cfa067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ79IFFAqJ6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del data['Unnamed: 32']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF447yPOqnjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "X = data.iloc[:,2:].values\n",
        "y = data.iloc[:,1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtmHRaMIy1qP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoding our labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "Labelencoder = LabelEncoder()\n",
        "y = Labelencoder.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow3OJulBzX-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size  = 0.1,random_state = 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hstNJjSczuy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalizing our data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5etQ1t0h1Jjk",
        "colab_type": "code",
        "outputId": "ac496861-3ed0-4c59-8e4c-7b315cc7e9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.52787029,  2.49821982, -0.59939466, ..., -1.74713139,\n",
              "        -0.79044533, -0.91054389],\n",
              "       [-0.55333608,  0.29431013, -0.60759343, ..., -0.62275667,\n",
              "        -0.33646358, -0.83551633],\n",
              "       [ 2.15452653,  0.40392257,  2.26525805, ...,  1.03846122,\n",
              "        -0.11504791,  0.26488788],\n",
              "       ...,\n",
              "       [-1.3297598 , -0.21876938, -1.32088704, ..., -0.98271999,\n",
              "        -0.718764  , -0.13637062],\n",
              "       [-1.24940108, -0.24209117, -1.2835826 , ..., -1.74713139,\n",
              "        -1.58690456, -1.01280367],\n",
              "       [-0.74291476,  1.08958336, -0.71827692, ..., -0.2865488 ,\n",
              "        -1.26354211,  0.19486216]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO1IJct81Qat",
        "colab_type": "code",
        "outputId": "f2e95977-0260-4c6e-d4ee-b850b6b6d8b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "X_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.19207516,  0.21655218, -0.11542614, ...,  1.60315155,\n",
              "         1.38812047,  1.39561107],\n",
              "       [-0.24797128,  1.29875111, -0.31114085, ..., -0.76400211,\n",
              "        -0.79834577, -0.93437281],\n",
              "       [-0.00967731, -0.89539007, -0.07447263, ..., -0.40317641,\n",
              "        -1.38855752, -0.97661799],\n",
              "       ...,\n",
              "       [-0.46861385,  0.07241152, -0.46935075, ..., -0.49962508,\n",
              "        -0.33652424, -0.40576652],\n",
              "       [-0.75986205, -0.81073603, -0.78878818, ..., -0.67538685,\n",
              "        -0.01842311,  0.13529668],\n",
              "       [-0.80399056,  1.71744542, -0.84310547, ..., -1.40405566,\n",
              "        -0.02225565, -0.77243298]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk7sIFwl1T1F",
        "colab_type": "code",
        "outputId": "d7dfb9fd-0ee2-4a74-f8b2-d07e83e360cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "! pip install keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i_v6KE11dR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv2BthDJ1tvt",
        "colab_type": "code",
        "outputId": "e96d70b7-9f4c-4bf5-db5e-267539f08758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "#adding the input and first hidden layer\n",
        "model = Sequential()\n",
        "model.add(Dense(output_dim = 16,init = 'uniform',activation = 'relu',input_dim = 30))\n",
        "\n",
        "#adding the second hidden layer\n",
        "model.add(Dense(output_dim = 16,init = 'uniform',activation = 'relu'))\n",
        "\n",
        "#adding the output layer\n",
        "model.add(Dense(output_dim = 1,init = 'uniform' ,activation = 'sigmoid'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkcDI4Zr20gc",
        "colab_type": "code",
        "outputId": "a042bd7b-e0a9-4c03-f281-a781d083f2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ8TBm953wIE",
        "colab_type": "code",
        "outputId": "e5025872-2d4e-4761-a2f4-0261dc8cfb3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X,y,batch_size = 100,epochs = 150)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "569/569 [==============================] - 0s 42us/step - loss: 0.4594 - acc: 0.9104\n",
            "Epoch 2/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.4126 - acc: 0.8910\n",
            "Epoch 3/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.3659 - acc: 0.9033\n",
            "Epoch 4/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.3259 - acc: 0.9051\n",
            "Epoch 5/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.2995 - acc: 0.9121\n",
            "Epoch 6/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.2812 - acc: 0.9069\n",
            "Epoch 7/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.2608 - acc: 0.9174\n",
            "Epoch 8/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.2621 - acc: 0.9016\n",
            "Epoch 9/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.2509 - acc: 0.9104\n",
            "Epoch 10/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.2430 - acc: 0.9209\n",
            "Epoch 11/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.2411 - acc: 0.9139\n",
            "Epoch 12/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.2249 - acc: 0.9016\n",
            "Epoch 13/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.2228 - acc: 0.9156\n",
            "Epoch 14/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.2303 - acc: 0.9121\n",
            "Epoch 15/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.2231 - acc: 0.9121\n",
            "Epoch 16/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.2171 - acc: 0.9051\n",
            "Epoch 17/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.2213 - acc: 0.9104\n",
            "Epoch 18/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.2171 - acc: 0.9139\n",
            "Epoch 19/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.2114 - acc: 0.9209\n",
            "Epoch 20/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.2112 - acc: 0.9139\n",
            "Epoch 21/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.2144 - acc: 0.9227\n",
            "Epoch 22/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.2075 - acc: 0.9244\n",
            "Epoch 23/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.2113 - acc: 0.9139\n",
            "Epoch 24/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.2099 - acc: 0.9209\n",
            "Epoch 25/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.2050 - acc: 0.9156\n",
            "Epoch 26/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.2070 - acc: 0.9139\n",
            "Epoch 27/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.2046 - acc: 0.9227\n",
            "Epoch 28/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.2039 - acc: 0.9192\n",
            "Epoch 29/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.2025 - acc: 0.9156\n",
            "Epoch 30/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1986 - acc: 0.9209\n",
            "Epoch 31/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.1977 - acc: 0.9244\n",
            "Epoch 32/150\n",
            "569/569 [==============================] - 0s 30us/step - loss: 0.1978 - acc: 0.9227\n",
            "Epoch 33/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1970 - acc: 0.9244\n",
            "Epoch 34/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.2102 - acc: 0.9174\n",
            "Epoch 35/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.2037 - acc: 0.9209\n",
            "Epoch 36/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.2040 - acc: 0.9227\n",
            "Epoch 37/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.2019 - acc: 0.9227\n",
            "Epoch 38/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.2051 - acc: 0.9139\n",
            "Epoch 39/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1960 - acc: 0.9192\n",
            "Epoch 40/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.2051 - acc: 0.9244\n",
            "Epoch 41/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.2052 - acc: 0.9192\n",
            "Epoch 42/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.2067 - acc: 0.9227\n",
            "Epoch 43/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.2054 - acc: 0.9209\n",
            "Epoch 44/150\n",
            "569/569 [==============================] - 0s 28us/step - loss: 0.1965 - acc: 0.9227\n",
            "Epoch 45/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.1951 - acc: 0.9244\n",
            "Epoch 46/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1904 - acc: 0.9227\n",
            "Epoch 47/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1879 - acc: 0.9227\n",
            "Epoch 48/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.2022 - acc: 0.9209\n",
            "Epoch 49/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.2017 - acc: 0.9227\n",
            "Epoch 50/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1942 - acc: 0.9262\n",
            "Epoch 51/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.1983 - acc: 0.9192\n",
            "Epoch 52/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.1946 - acc: 0.9209\n",
            "Epoch 53/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1949 - acc: 0.9279\n",
            "Epoch 54/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1842 - acc: 0.9244\n",
            "Epoch 55/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1863 - acc: 0.9227\n",
            "Epoch 56/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1850 - acc: 0.9244\n",
            "Epoch 57/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1837 - acc: 0.9262\n",
            "Epoch 58/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1834 - acc: 0.9262\n",
            "Epoch 59/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1835 - acc: 0.9262\n",
            "Epoch 60/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1833 - acc: 0.9209\n",
            "Epoch 61/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1807 - acc: 0.9227\n",
            "Epoch 62/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1867 - acc: 0.9297\n",
            "Epoch 63/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1948 - acc: 0.9244\n",
            "Epoch 64/150\n",
            "569/569 [==============================] - 0s 18us/step - loss: 0.1869 - acc: 0.9244\n",
            "Epoch 65/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.2016 - acc: 0.9192\n",
            "Epoch 66/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1882 - acc: 0.9227\n",
            "Epoch 67/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1816 - acc: 0.9279\n",
            "Epoch 68/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1847 - acc: 0.9244\n",
            "Epoch 69/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1773 - acc: 0.9262\n",
            "Epoch 70/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1794 - acc: 0.9297\n",
            "Epoch 71/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1855 - acc: 0.9279\n",
            "Epoch 72/150\n",
            "569/569 [==============================] - 0s 17us/step - loss: 0.1892 - acc: 0.9244\n",
            "Epoch 73/150\n",
            "569/569 [==============================] - 0s 18us/step - loss: 0.1876 - acc: 0.9244\n",
            "Epoch 74/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1777 - acc: 0.9279\n",
            "Epoch 75/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1735 - acc: 0.9297\n",
            "Epoch 76/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1747 - acc: 0.9279\n",
            "Epoch 77/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1757 - acc: 0.9244\n",
            "Epoch 78/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1738 - acc: 0.9279\n",
            "Epoch 79/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1730 - acc: 0.9279\n",
            "Epoch 80/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1730 - acc: 0.9297\n",
            "Epoch 81/150\n",
            "569/569 [==============================] - 0s 30us/step - loss: 0.1840 - acc: 0.9297\n",
            "Epoch 82/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1761 - acc: 0.9350\n",
            "Epoch 83/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1722 - acc: 0.9279\n",
            "Epoch 84/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1696 - acc: 0.9279\n",
            "Epoch 85/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1708 - acc: 0.9279\n",
            "Epoch 86/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1744 - acc: 0.9332\n",
            "Epoch 87/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1731 - acc: 0.9332\n",
            "Epoch 88/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1680 - acc: 0.9297\n",
            "Epoch 89/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1717 - acc: 0.9297\n",
            "Epoch 90/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1701 - acc: 0.9297\n",
            "Epoch 91/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1697 - acc: 0.9279\n",
            "Epoch 92/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1783 - acc: 0.9279\n",
            "Epoch 93/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1709 - acc: 0.9315\n",
            "Epoch 94/150\n",
            "569/569 [==============================] - 0s 17us/step - loss: 0.1688 - acc: 0.9279\n",
            "Epoch 95/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1680 - acc: 0.9332\n",
            "Epoch 96/150\n",
            "569/569 [==============================] - 0s 18us/step - loss: 0.1678 - acc: 0.9332\n",
            "Epoch 97/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1648 - acc: 0.9315\n",
            "Epoch 98/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1678 - acc: 0.9332\n",
            "Epoch 99/150\n",
            "569/569 [==============================] - 0s 17us/step - loss: 0.1666 - acc: 0.9315\n",
            "Epoch 100/150\n",
            "569/569 [==============================] - 0s 18us/step - loss: 0.1654 - acc: 0.9367\n",
            "Epoch 101/150\n",
            "569/569 [==============================] - 0s 18us/step - loss: 0.1646 - acc: 0.9262\n",
            "Epoch 102/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1634 - acc: 0.9385\n",
            "Epoch 103/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1646 - acc: 0.9367\n",
            "Epoch 104/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1668 - acc: 0.9332\n",
            "Epoch 105/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1767 - acc: 0.9279\n",
            "Epoch 106/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1637 - acc: 0.9350\n",
            "Epoch 107/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.1585 - acc: 0.9385\n",
            "Epoch 108/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1621 - acc: 0.9367\n",
            "Epoch 109/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1641 - acc: 0.9367\n",
            "Epoch 110/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1673 - acc: 0.9350\n",
            "Epoch 111/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1600 - acc: 0.9350\n",
            "Epoch 112/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1691 - acc: 0.9350\n",
            "Epoch 113/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1656 - acc: 0.9279\n",
            "Epoch 114/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1636 - acc: 0.9350\n",
            "Epoch 115/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1584 - acc: 0.9367\n",
            "Epoch 116/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1597 - acc: 0.9385\n",
            "Epoch 117/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1750 - acc: 0.9297\n",
            "Epoch 118/150\n",
            "569/569 [==============================] - 0s 27us/step - loss: 0.1673 - acc: 0.9297\n",
            "Epoch 119/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1839 - acc: 0.9279\n",
            "Epoch 120/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1724 - acc: 0.9262\n",
            "Epoch 121/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.1611 - acc: 0.9367\n",
            "Epoch 122/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1604 - acc: 0.9332\n",
            "Epoch 123/150\n",
            "569/569 [==============================] - 0s 30us/step - loss: 0.1573 - acc: 0.9332\n",
            "Epoch 124/150\n",
            "569/569 [==============================] - 0s 29us/step - loss: 0.1604 - acc: 0.9367\n",
            "Epoch 125/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.1604 - acc: 0.9367\n",
            "Epoch 126/150\n",
            "569/569 [==============================] - 0s 31us/step - loss: 0.1588 - acc: 0.9332\n",
            "Epoch 127/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1614 - acc: 0.9385\n",
            "Epoch 128/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1640 - acc: 0.9350\n",
            "Epoch 129/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1616 - acc: 0.9315\n",
            "Epoch 130/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1561 - acc: 0.9385\n",
            "Epoch 131/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1522 - acc: 0.9385\n",
            "Epoch 132/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1530 - acc: 0.9385\n",
            "Epoch 133/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1543 - acc: 0.9367\n",
            "Epoch 134/150\n",
            "569/569 [==============================] - 0s 19us/step - loss: 0.1574 - acc: 0.9350\n",
            "Epoch 135/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1540 - acc: 0.9350\n",
            "Epoch 136/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1580 - acc: 0.9402\n",
            "Epoch 137/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1633 - acc: 0.9402\n",
            "Epoch 138/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1570 - acc: 0.9350\n",
            "Epoch 139/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1682 - acc: 0.9315\n",
            "Epoch 140/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.1588 - acc: 0.9315\n",
            "Epoch 141/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1538 - acc: 0.9385\n",
            "Epoch 142/150\n",
            "569/569 [==============================] - 0s 24us/step - loss: 0.1502 - acc: 0.9350\n",
            "Epoch 143/150\n",
            "569/569 [==============================] - 0s 35us/step - loss: 0.1497 - acc: 0.9350\n",
            "Epoch 144/150\n",
            "569/569 [==============================] - 0s 21us/step - loss: 0.1503 - acc: 0.9367\n",
            "Epoch 145/150\n",
            "569/569 [==============================] - 0s 22us/step - loss: 0.1515 - acc: 0.9402\n",
            "Epoch 146/150\n",
            "569/569 [==============================] - 0s 23us/step - loss: 0.1488 - acc: 0.9367\n",
            "Epoch 147/150\n",
            "569/569 [==============================] - 0s 25us/step - loss: 0.1490 - acc: 0.9402\n",
            "Epoch 148/150\n",
            "569/569 [==============================] - 0s 30us/step - loss: 0.1481 - acc: 0.9367\n",
            "Epoch 149/150\n",
            "569/569 [==============================] - 0s 26us/step - loss: 0.1515 - acc: 0.9402\n",
            "Epoch 150/150\n",
            "569/569 [==============================] - 0s 20us/step - loss: 0.1546 - acc: 0.9402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faa5aff75f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF7AVelW4SmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwZgzvyp4iVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXTIWm0A4nKl",
        "colab_type": "code",
        "outputId": "be8e9b89-28f2-41cf-ca06-51412a8b236d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cm,annot=True)\n",
        "plt.savefig('h.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD4CAYAAACt8i4nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmCEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S560UXay5Ubs",
        "colab_type": "code",
        "outputId": "59df5061-60ba-475c-a6c2-780822fb03f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(29+16)/51"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8823529411764706"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H9lMcIu6yoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}